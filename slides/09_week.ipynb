{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import seaborn as sns\n",
    "import sklearn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using CNTK backend\n"
     ]
    }
   ],
   "source": [
    "import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "from keras import layers, models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## COMP 3132 -  Machine Learning with Python\n",
    "__Week 9 lecture__\n",
    "\n",
    "### [github.com/kamrik/ML2](https://github.com/kamrik/ML2)\n",
    "\n",
    "### [slido.com/COMP3132](http://slido.com/COMP3132)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "\n",
    "## Plan for today\n",
    " - Home assignment questions (if any)\n",
    " - Recap of the first half of the course\n",
    " - Deep learning for images - Convnets (chapter 5, till & including 5.2)\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Home assignment questions?\n",
    " - Due March 13\n",
    " - Start with the Yelp lab from previous term and newswires example from the book.\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Recap"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## ML models - whether NNs or not\n",
    " - Come up with a model\n",
    " - Train / fit it by showing examples\n",
    " - Use it to predict and evaluate how good the prediction is"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Neuron\n",
    "![neuron](https://naadispeaks.files.wordpress.com/2017/11/vqope.jpg?w=630)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Inspired by, but not an accurate model of the real neuron\n",
    "![real neuron](https://upload.wikimedia.org/wikipedia/commons/4/44/Neuron3.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Layers\n",
    "![layes](https://upload.wikimedia.org/wikipedia/commons/4/46/Colored_neural_network.svg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "![more layers](https://www.pythonbots.ml/static/images/sw.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "![learning](learning_dagram_fig3_1.png)\n",
    "Figure 3.1, from the [book](https://www.manning.com/books/deep-learning-with-python)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## ConvNets\n",
    " - So far we used Dense layers\n",
    " - But for a picture of decent size like 1 MegaPixel that's too many connections\n",
    " - In ConvNets a unit is not connected to all units in the next layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Biological inspiration\n",
    " - Work by David Hubel & Torsten Wiesel in the 1960s showed that cat and monkey visual cortexes contain neurons that individually respond to small regions of the visual field [[wikipedia](https://en.wikipedia.org/wiki/Convolutional_neural_network#History)]\n",
    " - https://www.ncbi.nlm.nih.gov/pmc/articles/PMC1557912/\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "![ConvNet](https://upload.wikimedia.org/wikipedia/commons/6/68/Conv_layer.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "![layers](https://upload.wikimedia.org/wikipedia/commons/8/8a/Conv_layers.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Example using MNIST digits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAUsAAAFJCAYAAAASfw+VAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n/g9PVu55KTjAQAAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1db055a4240>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from keras.datasets import mnist\n",
    "(train_images, train_labels), (test_images, test_labels) = mnist.load_data()\n",
    "\n",
    "print(train_labels[7])\n",
    "ax = plt.imshow(train_images[7], cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Some boring preprocessing\n",
    "train_images = train_images.reshape((60000, 28, 28, 1))\n",
    "train_images = train_images.astype('float32') / 255\n",
    "\n",
    "test_images = test_images.reshape((10000, 28, 28, 1))\n",
    "test_images = test_images.astype('float32') / 255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.,  0.,  0.,  1.,  0.,  0.,  0.,  0.,  0.,  0.], dtype=float32)"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# One-hot encode the labels (targets)\n",
    "from keras.utils import to_categorical\n",
    "train_labels = to_categorical(train_labels).astype('float32')\n",
    "test_labels = to_categorical(test_labels).astype('float32')\n",
    "\n",
    "train_labels[7]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## The model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras import layers\n",
    "from keras import models\n",
    "\n",
    "model = models.Sequential()\n",
    "model.add(layers.Conv2D(32, (3, 3), activation='relu', input_shape=(28, 28, 1)))\n",
    "model.add(layers.MaxPooling2D((2, 2)))\n",
    "model.add(layers.Conv2D(64, (3, 3), activation='relu'))\n",
    "model.add(layers.MaxPooling2D((2, 2)))\n",
    "model.add(layers.Conv2D(64, (3, 3), activation='relu'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## MaxPooling\n",
    "![maxpooling](https://computersciencewiki.org/images/8/8a/MaxpoolSample2.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_7 (Conv2D)            (None, 26, 26, 32)        320       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_5 (MaxPooling2 (None, 13, 13, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_8 (Conv2D)            (None, 11, 11, 64)        18496     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_6 (MaxPooling2 (None, 5, 5, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_9 (Conv2D)            (None, 3, 3, 64)          36928     \n",
      "=================================================================\n",
      "Total params: 55,744\n",
      "Trainable params: 55,744\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Dense layers on top of the Conv2D ones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "model.add(layers.Flatten())\n",
    "model.add(layers.Dense(64, activation='relu'))\n",
    "model.add(layers.Dense(10, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_7 (Conv2D)            (None, 26, 26, 32)        320       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_5 (MaxPooling2 (None, 13, 13, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_8 (Conv2D)            (None, 11, 11, 64)        18496     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_6 (MaxPooling2 (None, 5, 5, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_9 (Conv2D)            (None, 3, 3, 64)          36928     \n",
      "_________________________________________________________________\n",
      "flatten_4 (Flatten)          (None, 576)               0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 64)                36928     \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 10)                650       \n",
      "=================================================================\n",
      "Total params: 93,322\n",
      "Trainable params: 93,322\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "model.compile(optimizer='rmsprop',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/5\n",
      "60000/60000 [==============================] - 14s - loss: 0.0156 - acc: 0.9951 - val_loss: 0.0268 - val_acc: 0.9930\n",
      "Epoch 2/5\n",
      "60000/60000 [==============================] - 14s - loss: 0.0131 - acc: 0.9959 - val_loss: 0.0281 - val_acc: 0.9926\n",
      "Epoch 3/5\n",
      "60000/60000 [==============================] - 15s - loss: 0.0109 - acc: 0.9970 - val_loss: 0.0311 - val_acc: 0.9924\n",
      "Epoch 4/5\n",
      "60000/60000 [==============================] - 14s - loss: 0.0095 - acc: 0.9972 - val_loss: 0.0354 - val_acc: 0.9904\n",
      "Epoch 5/5\n",
      "60000/60000 [==============================] - 14s - loss: 0.0087 - acc: 0.9974 - val_loss: 0.0450 - val_acc: 0.9904\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1db058bf320>"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(train_images, train_labels,\n",
    "          epochs=5, batch_size=64, \n",
    "          validation_data=(test_images, test_labels)\n",
    "         )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
